#!/usr/bin/env python3
"""
MAL Import to Cloud Database - COMPLETE RICH DATA VERSION
Imports COMPLETE MAL data to the CLOUD Supabase database using intelligent ID discovery
Features:
- ALL available MAL API fields (alternative titles, pictures, relations, recommendations, etc.)
- Enhanced categorization (genres, themes, demographics)
- Opening/ending themes for anime
- Detailed statistics and metadata
- Smart ID discovery for optimal performance
- Excludes NSFW content as requested
- ROBUST: Includes retry logic for Supabase network connections
- OPTIMIZED: Atomic upserts, consolidated code, bulletproof error handling
"""

import os
from dotenv import load_dotenv

# Load environment variables (should have both MAL and Supabase cloud credentials)
load_dotenv()

# Ensure we use CLOUD database (not local)
os.environ['LOCAL_DEVELOPMENT'] = 'False'  # Use cloud database
os.environ['USE_LOCAL_SUPABASE'] = 'False'  # Use cloud database

import asyncio
import time
import json
from datetime import datetime
from typing import List, Dict, Tuple, Set, Optional
import requests  # Added for specific exception handling
from mal_api_client import MALAPIClient, MALDataTransformer
from supabase_client import SupabaseClient

class OptimizedCloudMALImporter:
    """COMPLETE MAL importer with ALL rich data fields and enhanced categorization"""
    
    def __init__(self):
        print("üöÄ Initializing COMPLETE Cloud MAL Importer with ALL Rich Data")
        print("   Features: ALL MAL API Fields + Enhanced Categorization + Smart ID Discovery")
        print("   Rich Data: Alternative Titles, Pictures, Relations, Recommendations, Statistics, Themes")
        print("   Target: CLOUD Supabase Database")
        print()
        
        self.mal_client = MALAPIClient()
        self.supabase = SupabaseClient()
        
        # Verify we're connecting to cloud database
        env_info = self.get_environment_info()
        print(f"üìä Database Configuration:")
        print(f"   Environment: {env_info['environment']}")
        print(f"   Supabase URL: {env_info['supabase_url']}")
        print(f"   Database Type: {env_info['database_type']}")
        print()
        
        # Progress tracking
        self.anime_imported = 0
        self.manga_imported = 0
        self.anime_failed = 0
        self.manga_failed = 0
        
        # Conservative performance settings for MAL's dynamic rate limiting
        self.batch_size = 50  # Keep small batches for easier recovery
        self.concurrent_requests = 1 # Sequential processing to avoid overwhelming MAL
        self.delay_between_requests = 0.1# Minimal delay since MAL client handles rate limiting
        
        # MAL cooldown management
        self.request_timeout = 30  # 30 second timeout per request
        self.max_retries = 1  # Quick failure since cooldowns are the main issue
        
        # Remove the progressive backoff since MAL client now handles this
        self.mal_requests_made = 0
        
        # State management for resumable imports
        self.state_file = 'mal_optimized_import_state.json'
        self.state = self.load_state()
        
        # SMART ID discovery cache
        self.discovered_anime_ids: Set[int] = set()
        self.discovered_manga_ids: Set[int] = set()
        self.id_discovery_cache_file = 'mal_discovered_ids.json'
        self.load_discovered_ids()
        
        # Enhanced data categorization system
        self.genre_categories = {
            'genres': {
                1: 'Action', 2: 'Adventure', 4: 'Comedy', 8: 'Drama', 10: 'Fantasy', 
                14: 'Horror', 18: 'Mecha', 19: 'Music', 22: 'Romance', 24: 'Sci-Fi',
                30: 'Sports', 37: 'Supernatural', 38: 'Military', 39: 'Police',
                40: 'Psychological', 41: 'Thriller', 9: 'Ecchi', 12: 'Hentai',
                35: 'Harem', 36: 'Slice of Life', 46: 'Award Winning'
            },
            'themes': {
                # Themes (story/setting elements)
                13: 'Historical', 17: 'Martial Arts', 21: 'Samurai', 23: 'School',
                31: 'Super Power', 32: 'Vampire', 33: 'Yaoi', 34: 'Yuri',
                58: 'Gore', 62: 'Isekai', 63: 'Love Polygon', 64: 'Love Triangle',
                66: 'Mythology', 67: 'Organized Crime', 68: 'Otaku Culture', 
                69: 'Parody', 70: 'Performing Arts', 71: 'Pets', 72: 'Reincarnation', 
                73: 'Reverse Harem', 74: 'Romantic Subtext', 75: 'Showbiz', 
                76: 'Survival', 77: 'Team Sports', 78: 'Time Travel', 79: 'Video Game',
                80: 'Visual Arts', 81: 'Workplace', 82: 'Crossdressing',
                83: 'Delinquents', 84: 'Gag Humor', 85: 'CGDCT'
            },
            'demographics': {
                # Demographics (target audience)
                15: 'Kids', 25: 'Shoujo', 27: 'Shounen', 42: 'Seinen', 43: 'Josei'
            }
        }
        
        # Reverse mapping for quick lookup
        self.category_lookup = {}
        for category, items in self.genre_categories.items():
            for genre_id, name in items.items():
                self.category_lookup[name.lower()] = category
        
        # Note: ensure_media_types() is now async and will be called at the start of import
        
        print("üé® Enhanced Data Categorization:")
        print(f"   Genres: {len(self.genre_categories['genres'])} traditional genres")
        print(f"   Themes: {len(self.genre_categories['themes'])} story/setting themes")
        print(f"   Demographics: {len(self.genre_categories['demographics'])} target audiences")
        print()
    
    def get_environment_info(self):
        """Get current environment information"""
        supabase_url = os.getenv('SUPABASE_URL', 'Not set')
        
        if '127.0.0.1' in supabase_url or 'localhost' in supabase_url:
            return {
                'environment': 'LOCAL',
                'supabase_url': supabase_url,
                'database_type': 'Local PostgreSQL'
            }
        else:
            return {
                'environment': 'CLOUD',
                'supabase_url': supabase_url,
                'database_type': 'Supabase Cloud'
            }
    
    async def _make_supabase_request_with_retry(self, method: str, table: str, data: Optional[Dict] = None, params: Optional[Dict] = None):
        """A robust wrapper for Supabase requests that handles transient network/server errors."""
        max_retries = 5
        initial_backoff = 2  # Start with a 2-second delay
        
        for attempt in range(max_retries):
            try:
                response = self.supabase._make_request(method, table, data=data, params=params)
                response.raise_for_status()  # Raises HTTPError for 4xx/5xx responses
                return response  # Success
            except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.ChunkedEncodingError) as e:
                print(f"      üö® Supabase Network Error (Attempt {attempt + 1}/{max_retries}): {e}")
            except requests.exceptions.HTTPError as e:
                # Only retry on server errors (5xx), not client errors (4xx)
                if 500 <= e.response.status_code < 600:
                    print(f"      üö® Supabase Server Error {e.response.status_code} (Attempt {attempt + 1}/{max_retries})")
                else:
                    print(f"      ‚ùå Supabase Client Error {e.response.status_code}. Not retrying.")
                    raise  # Do not retry on 4xx errors
            
            if attempt == max_retries - 1:
                print(f"      ‚ùå Supabase request failed after {max_retries} attempts. Giving up.")
                raise
            
            backoff_time = initial_backoff * (2 ** attempt)
            print(f"      ‚è≥ Retrying in {backoff_time} seconds...")
            await asyncio.sleep(backoff_time)
            
        raise Exception("Supabase request failed after all retries.")
    
    async def _upsert_entity_with_retry(self, table_name: str, data: dict) -> Optional[dict]:
        """
        Performs a robust upsert operation using the existing Supabase client.
        Falls back to get-then-post pattern since the client doesn't support custom headers.
        """
        try:
            # First, try to find existing entity
            response = await self._make_supabase_request_with_retry('GET', table_name, params={'select': '*', 'name': f"eq.{data['name']}"})
            entities = response.json()
            
            if entities:
                # Entity already exists, return it
                return entities[0]
            
            # Entity doesn't exist, create it
            try:
                response = await self._make_supabase_request_with_retry('POST', table_name, data=data)
                result = response.json()
                
                if isinstance(result, list) and result:
                    return result[0]
                elif isinstance(result, dict):
                    return result
                else:
                    # If creation failed, try to get it again (race condition handling)
                    response = await self._make_supabase_request_with_retry('GET', table_name, params={'select': '*', 'name': f"eq.{data['name']}"})
                    entities = response.json()
                    return entities[0] if entities else None
                    
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 409:  # Conflict - entity was created concurrently
                    # Try to fetch the entity that was created by another process
                    response = await self._make_supabase_request_with_retry('GET', table_name, params={'select': '*', 'name': f"eq.{data['name']}"})
                    entities = response.json()
                    return entities[0] if entities else None
                else:
                    raise
                    
        except Exception as e:
            print(f"      ‚ö†Ô∏è  Could not upsert entity in '{table_name}' with data {data}: {e}")
            return None
    
    def load_discovered_ids(self):
        """Load previously discovered valid MAL IDs"""
        try:
            if os.path.exists(self.id_discovery_cache_file):
                with open(self.id_discovery_cache_file, 'r') as f:
                    data = json.load(f)
                    self.discovered_anime_ids = set(data.get('anime_ids', []))
                    self.discovered_manga_ids = set(data.get('manga_ids', []))
                    print(f"üìã Loaded cached IDs: {len(self.discovered_anime_ids):,} anime, {len(self.discovered_manga_ids):,} manga")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not load ID cache: {e}")
    
    def save_discovered_ids(self):
        """Save discovered valid MAL IDs for future runs"""
        try:
            data = {
                'anime_ids': sorted(list(self.discovered_anime_ids)),
                'manga_ids': sorted(list(self.discovered_manga_ids)),
                'last_update': time.time()
            }
            with open(self.id_discovery_cache_file, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not save ID cache: {e}")
    
    async def discover_valid_ids_via_ranking(self, media_type: str, max_pages: int = 50) -> Set[int]:
        """Discover valid MAL IDs using ranking API - much more efficient than sequential scanning"""
        print(f"üéØ Discovering valid {media_type} IDs via MAL ranking API...")
        
        discovered_ids = set()
        
        # Use the comprehensive ranking API (already uses ranking_type=all internally)
        print(f"   üìä Scanning {media_type} ranking via MAL API...")
        
        for page in range(max_pages):
            offset = page * 500  # MAL's max per request
            
            try:
                if media_type == 'anime':
                    ranking_data = await self.mal_client.get_anime_list(
                        limit=500,
                        offset=offset
                    )
                else:
                    ranking_data = await self.mal_client.get_manga_list(
                        limit=500,
                        offset=offset
                    )
                
                if not ranking_data or not ranking_data.get('data'):
                    print(f"      üìã No more data at offset {offset:,} - reached end")
                    break
                
                batch_ids = []
                for item in ranking_data['data']:
                    item_id = item['node']['id']
                    discovered_ids.add(item_id)
                    batch_ids.append(item_id)
                
                print(f"       Page {page + 1}: +{len(batch_ids)} IDs (total: {len(discovered_ids):,})")
                
                # If we got less than the limit, we've reached the end
                if len(ranking_data['data']) < 500:
                    print(f"      üìã Got {len(ranking_data['data'])} items (< 500) - reached end")
                    break
                    
                # Conservative delay to prevent timeouts
                await asyncio.sleep(1.0)
                
            except Exception as e:
                print(f"      ‚ö†Ô∏è  Error at offset {offset}: {e}")
                break
        
        print(f"   üéâ Discovered {len(discovered_ids):,} valid {media_type} IDs via ranking API")
        return discovered_ids
    
    async def discover_additional_ids_via_search(self, media_type: str, search_terms: List[str]) -> Set[int]:
        """Use search API to discover additional IDs not found in rankings"""
        print(f"üîç Discovering additional {media_type} IDs via search...")
        
        discovered_ids = set()
        
        # Popular search terms to find additional content
        popular_terms = search_terms + [
            "One Piece", "Naruto", "Dragon Ball", "Attack on Titan", "Death Note", 
            "Fullmetal Alchemist", "My Hero Academia", "Demon Slayer", "Hunter x Hunter",
            "Tokyo Ghoul", "Bleach", "Sword Art Online", "JoJo", "Code Geass",
            "Evangelion", "Cowboy Bebop", "Studio Ghibli", "Pokemon", "Digimon"
        ]
        
        for term in popular_terms:
            try:
                if media_type == 'anime':
                    results = await self.mal_client.search_anime(term, limit=100)
                else:
                    results = await self.mal_client.search_manga(term, limit=100)
                
                if results and results.get('data'):
                    batch_ids = []
                    for item in results['data']:
                        item_id = item['node']['id']
                        discovered_ids.add(item_id)
                        batch_ids.append(item_id)
                    
                    if batch_ids:
                        print(f"   üìù '{term}': +{len(batch_ids)} IDs")
                
                await asyncio.sleep(1.0)  # Conservative delay to prevent timeouts
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Search error for '{term}': {e}")
                continue
        
        print(f"   üîç Search discovered {len(discovered_ids):,} additional {media_type} IDs")
        return discovered_ids
    
    async def check_existing_items(self):
        """Check what's already in the cloud database"""
        try:
            print("üîç Checking existing items in cloud database...")
            
            # Get all items to check what exists
            all_items = self.supabase.get_all_items_paginated()
            
            anime_count = sum(1 for item in all_items if item.get('media_type_id') == 1)
            manga_count = sum(1 for item in all_items if item.get('media_type_id') == 2)
            mal_items = sum(1 for item in all_items if item.get('mal_id') is not None)
            
            print(f"   Total items: {len(all_items):,}")
            print(f"   Anime: {anime_count:,}")
            print(f"   Manga: {manga_count:,}")
            print(f"   Items with MAL ID: {mal_items:,}")
            
            return {
                'total': len(all_items),
                'anime': anime_count,
                'manga': manga_count,
                'mal_items': mal_items
            }
            
        except Exception as e:
            print(f"‚ùå Error checking existing items: {e}")
            return None
    
    async def ensure_media_types(self):
        """Ensure media types exist in the database"""
        print("   Checking/creating 'anime' and 'manga' media types...")
        try:
            await self._upsert_entity_with_retry('media_types', {'id': 1, 'name': 'anime'})
            await self._upsert_entity_with_retry('media_types', {'id': 2, 'name': 'manga'})
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not ensure media types: {e}")
        print("   ...done.")
    
    def load_state(self):
        """Load import state from file"""
        try:
            if os.path.exists(self.state_file):
                with open(self.state_file, 'r') as f:
                    return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not load state file: {e}")
        
        # Default state
        return {
            'anime_processed_ids': [],
            'manga_processed_ids': [],
            'anime_successful': 0,
            'manga_successful': 0,
            'anime_failed': 0,
            'manga_failed': 0,
            'last_update': time.time()
        }
    
    def save_state(self):
        """Save current import state"""
        try:
            self.state.update({
                'anime_successful': self.anime_imported,
                'manga_successful': self.manga_imported,
                'anime_failed': self.anime_failed,
                'manga_failed': self.manga_failed,
                'last_update': time.time()
            })
            
            with open(self.state_file, 'w') as f:
                json.dump(self.state, f, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not save state: {e}")
    
    async def import_batch_concurrent_optimized(self, ids: List[int], media_type: str) -> Tuple[int, int, int]:
        """OPTIMIZED import with connection pooling and transaction fixes"""
        successful = 0
        failed = 0
        skipped = 0
        
        # Split into smaller concurrent groups
        semaphore = asyncio.Semaphore(self.concurrent_requests)
        
        async def import_single(item_id):
            async with semaphore:
                try:
                    # Use unified import method that determines actual type from MAL
                    success, message = await self.import_mal_item_unified(item_id)
                    
                    if success:
                        self.mal_requests_made = 0  # Reset request counter on success
                        return 1, 0, 0  # success, failure, skipped
                    else:
                        # Check for timeout issues
                        if "timeout" in message.lower():
                            self.mal_requests_made += 1
                            if self.mal_requests_made >= self.max_retries:
                                # Apply progressive backoff
                                backoff_delay = min(5.0, self.mal_requests_made * 1.0)
                                print(f"         üêå Too many timeouts, backing off for {backoff_delay}s...")
                                await asyncio.sleep(backoff_delay)
                        
                        # Distinguish between fetch failures (normal) and actual errors
                        if "Failed to fetch" in message or "not found on MAL" in message:
                            # This is normal - item doesn't exist on MAL
                            return 0, 0, 1  # success, failure, skipped
                        elif "already exists" in message:
                            # Item already exists in database - this should be skipped, not counted as success
                            print(f"         ‚è≠Ô∏è  Item {item_id} already exists in database")
                            return 0, 0, 1  # success, failure, skipped (NOT SUCCESS!)
                        else:
                            # This is an actual error
                            print(f"         ‚ùå Error for item {item_id}: {message}")
                            return 0, 1, 0  # success, failure, skipped
                        
                except Exception as e:
                    return 0, 1, 0  # success, failure, skipped
                finally:
                    # Dynamic delay based on timeout situation
                    delay = self.delay_between_requests
                    if self.mal_requests_made > 0:
                        delay *= (1 + self.mal_requests_made * 0.5)  # Increase delay if timeouts
                    await asyncio.sleep(delay)
            
            # after every import_single()
            if self.mal_client.requests_since_cooldown > 250:      # getting close
                self.delay_between_requests = 0.12                 # slow down a bit
            elif self.mal_client.requests_since_cooldown < 150:    # plenty of room
                self.delay_between_requests = max(0.05, self.delay_between_requests - 0.01)
        
        # Run imports (now sequential due to MAL's rate limiting)
        tasks = [import_single(item_id) for item_id in ids]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Check if MAL client is in cooldown and report status
        if hasattr(self.mal_client, 'is_in_cooldown') and self.mal_client.is_in_cooldown:
            cooldown_time = time.time() - self.mal_client.cooldown_start_time
            print(f"      üïê MAL Rate Limit Cooldown Active ({cooldown_time:.0f}s elapsed)")
            print(f"      üìä Requests made in this cycle: {self.mal_client.requests_since_cooldown}")
        
        # Count results efficiently
        for result in results:
            if isinstance(result, tuple) and len(result) == 3:
                s, f, sk = result
                successful += s
                failed += f
                skipped += sk
            else:
                failed += 1  # Exception occurred
        
        return successful, failed, skipped
    

    
    async def insert_enhanced_relationships_for_item(self, item_uid: str, categorized: Dict[str, List[str]], transformed: Dict):
        """Insert all categorized relationships for an item with enhanced data"""
        try:
            # Get item ID first
            response = await self._make_supabase_request_with_retry('GET', 'items', params={'select': 'id', 'uid': f'eq.{item_uid}'})
            items = response.json()
            if not items:
                return
            
            item_id = items[0]['id']
            
            # Helper function to process relationships with robust upsert
            async def process_relation(table_name, entity_name, join_table, fk_name, entity_id_name):
                if not entity_name or not entity_name.strip():
                    return
                entity = await self._upsert_entity_with_retry(table_name, {'name': entity_name.strip()})
                if entity:
                    try:
                        await self._make_supabase_request_with_retry('POST', join_table, data={fk_name: item_id, entity_id_name: entity['id']})
                    except requests.exceptions.HTTPError as e:
                        if e.response.status_code == 409:
                            pass  # Conflict/duplicate is OK
                        else:
                            raise
            
            # Process all relationships using the helper
            for genre_name in categorized.get('genres', []):
                await process_relation('genres', genre_name, 'item_genres', 'item_id', 'genre_id')
                
            for theme_name in categorized.get('themes', []):
                await process_relation('themes', theme_name, 'item_themes', 'item_id', 'theme_id')
                
            for demo_name in categorized.get('demographics', []):
                await process_relation('demographics', demo_name, 'item_demographics', 'item_id', 'demographic_id')
            
            # Insert studios (for anime)
            studios = transformed.get('studios', [])
            for studio_name in studios:
                await process_relation('studios', studio_name, 'item_studios', 'item_id', 'studio_id')
            
            # Insert authors (for manga)
            authors = transformed.get('authors', [])
            for author_name in authors:
                await process_relation('authors', author_name, 'item_authors', 'item_id', 'author_id')
                        
        except Exception as e:
            print(f"‚ö†Ô∏è  Error inserting enhanced relationships: {e}")
    
    async def insert_additional_rich_data(self, item_uid: str, mal_data: dict, media_type: str):
        """Insert additional rich data like pictures, relations, recommendations, statistics, and themes"""
        try:
            # Get item ID first
            response = await self._make_supabase_request_with_retry('GET', 'items', params={'select': 'id', 'uid': f'eq.{item_uid}'})
            items = response.json()
            if not items:
                return
            
            item_id = items[0]['id']
            
            # Helper to handle inserts and ignore 409 Conflict errors
            async def insert_ignore_conflict(table, data):
                try:
                    await self._make_supabase_request_with_retry('POST', table, data=data)
                except requests.exceptions.HTTPError as e:
                    if e.response.status_code == 409:
                        pass  # Ignore duplicates
                    else:
                        raise
            
            # 1. Insert additional pictures
            pictures = mal_data.get('pictures', [])
            for picture in pictures:
                if picture.get('large'):
                    await insert_ignore_conflict('item_pictures', {
                        'item_id': item_id,
                        'picture_url': picture['large'],
                        'picture_type': 'additional'
                    })
            
            # 2. Insert related anime/manga
            related_anime = mal_data.get('related_anime', [])
            related_manga = mal_data.get('related_manga', [])
            
            for related in related_anime + related_manga:
                if related.get('node', {}).get('id'):
                    await insert_ignore_conflict('item_relations', {
                        'item_id': item_id,
                        'related_item_mal_id': related['node']['id'],
                        'relation_type': related.get('relation_type', 'related')
                    })
            
            # 3. Insert recommendations
            recommendations = mal_data.get('recommendations', [])
            for rec in recommendations:
                if rec.get('node', {}).get('id'):
                    await insert_ignore_conflict('item_recommendations', {
                        'item_id': item_id,
                        'recommended_item_mal_id': rec['node']['id'],
                        'recommendation_count': rec.get('num_recommendations', 1)
                    })
            
            # 4. Insert statistics
            statistics = mal_data.get('statistics', {})
            if statistics:
                status_stats = statistics.get('status', {})
                if status_stats:
                    stats_data = {
                        'item_id': item_id,
                        'completed': status_stats.get('completed', 0),
                        'on_hold': status_stats.get('on_hold', 0),
                        'dropped': status_stats.get('dropped', 0)
                    }
                    
                    if media_type == 'anime':
                        stats_data.update({
                            'watching': status_stats.get('watching', 0),
                            'plan_to_watch': status_stats.get('plan_to_watch', 0)
                        })
                    else:  # manga
                        stats_data.update({
                            'reading': status_stats.get('reading', 0),
                            'plan_to_read': status_stats.get('plan_to_read', 0)
                        })
                    
                    await insert_ignore_conflict('item_statistics', stats_data)
            
            # 5. Insert opening/ending themes (anime only)
            if media_type == 'anime':
                # Opening themes
                opening_themes = mal_data.get('opening_themes', [])
                for theme in opening_themes:
                    if isinstance(theme, str) and theme.strip():
                        # Extract theme name (before any episode info)
                        theme_name = theme.split('(')[0].strip()
                        if theme_name:
                            # Insert or get theme using robust upsert
                            theme_entity = await self._upsert_entity_with_retry('opening_themes', {'name': theme_name})
                            if theme_entity:
                                await insert_ignore_conflict('item_opening_themes', {
                                    'item_id': item_id,
                                    'opening_theme_id': theme_entity['id'],
                                    'theme_text': theme
                                })
                
                # Ending themes
                ending_themes = mal_data.get('ending_themes', [])
                for theme in ending_themes:
                    if isinstance(theme, str) and theme.strip():
                        # Extract theme name (before any episode info)
                        theme_name = theme.split('(')[0].strip()
                        if theme_name:
                            # Insert or get theme using robust upsert
                            theme_entity = await self._upsert_entity_with_retry('ending_themes', {'name': theme_name})
                            if theme_entity:
                                await insert_ignore_conflict('item_ending_themes', {
                                    'item_id': item_id,
                                    'ending_theme_id': theme_entity['id'],
                                    'theme_text': theme
                                })
                            
        except Exception as e:
            print(f"‚ö†Ô∏è  Error inserting additional rich data: {e}")
    
    async def import_discovered_ids(self, id_list: List[int], media_type: str):
        """Import a specific list of IDs"""
        total_ids = len(id_list)
        
        # Process in batches
        for i in range(0, total_ids, self.batch_size):
            batch = id_list[i:i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            total_batches = (total_ids + self.batch_size - 1) // self.batch_size
            
            print(f"   üì¶ {media_type.title()} Batch {batch_num}/{total_batches} (IDs {batch[0]}-{batch[-1]})")
            
            successful, failed, skipped = await self.import_batch_concurrent_optimized(batch, media_type)
            
            if media_type == 'anime':
                self.anime_imported += successful
                self.anime_failed += failed
            else:
                self.manga_imported += successful
                self.manga_failed += failed
            
            if failed > 0:
                print(f"      ‚úÖ {successful} actually imported, ‚ùå {failed} failed, ‚è≠Ô∏è {skipped} skipped/not found")
            else:
                print(f"      ‚úÖ {successful} actually imported, ‚è≠Ô∏è {skipped} skipped/not found")
            print(f"      üìä Total {media_type}: {self.anime_imported if media_type == 'anime' else self.manga_imported:,} imported")
            print()
            
            # Update processed IDs for resume capability
            if media_type == 'anime':
                if 'anime_processed_ids' not in self.state:
                    self.state['anime_processed_ids'] = []
                self.state['anime_processed_ids'].extend(batch)
            else:
                if 'manga_processed_ids' not in self.state:
                    self.state['manga_processed_ids'] = []
                self.state['manga_processed_ids'].extend(batch)
            
            # Save progress
            self.save_state()
    
    async def import_id_list_with_label(self, id_list: List[int], media_type: str):
        """Import a list of IDs with clear labeling for concurrent processing"""
        print(f"üé¨ Starting {media_type} import ({len(id_list):,} discovered IDs)...")
        await self.import_discovered_ids(id_list, media_type)
        print(f"‚úÖ {media_type.title()} import completed!")
    
    async def start_optimized_smart_import(self):
        """Start OPTIMIZED MAL import using smart ID discovery + sequential scanning for completeness"""
        print("üöÄ Starting COMPREHENSIVE MAL Import to Cloud Database")
        print("   Phase 1: Smart ID Discovery (fast, high success rate)")
        print("   Phase 2: Sequential Scanning (comprehensive coverage)")
        print("=" * 70)
        
        # Ensure media types exist in the database
        print("üîß Setting up database schema...")
        await self.ensure_media_types()
        
        # Check existing items first
        existing = await self.check_existing_items()
        if existing is None:
            print("‚ùå Failed to check existing items. Aborting.")
            return
        
        start_time = time.time()
        
        print("üéØ Phase 1: Smart ID Discovery")
        print("=" * 40)
        
        # Discover valid anime IDs if not already cached
        if len(self.discovered_anime_ids) < 1000:  # If we don't have enough cached IDs
            print("üé¨ Discovering valid anime IDs...")
            anime_ids_ranking = await self.discover_valid_ids_via_ranking('anime', max_pages=100)
            anime_ids_search = await self.discover_additional_ids_via_search('anime', ["anime", "series", "movie"])
            self.discovered_anime_ids = anime_ids_ranking.union(anime_ids_search)
            print(f"   üéâ Total anime IDs discovered: {len(self.discovered_anime_ids):,}")
        else:
            print(f"   üìã Using cached anime IDs: {len(self.discovered_anime_ids):,}")
        
        # Discover valid manga IDs if not already cached
        if len(self.discovered_manga_ids) < 1000:  # If we don't have enough cached IDs
            print("üìö Discovering valid manga IDs...")
            manga_ids_ranking = await self.discover_valid_ids_via_ranking('manga', max_pages=100)
            manga_ids_search = await self.discover_additional_ids_via_search('manga', ["manga", "novel", "manhwa"])
            self.discovered_manga_ids = manga_ids_ranking.union(manga_ids_search)
            print(f"   üéâ Total manga IDs discovered: {len(self.discovered_manga_ids):,}")
        else:
            print(f"   üìã Using cached manga IDs: {len(self.discovered_manga_ids):,}")
        
        # Save discovered IDs for future runs
        self.save_discovered_ids()
        
        print()
        print("üéØ Phase 1: Smart Discovery Import")
        print("=" * 40)
        
        # Convert to sorted lists for processing
        anime_ids = sorted(list(self.discovered_anime_ids))
        manga_ids = sorted(list(self.discovered_manga_ids))
        
        # Load state for resume capability
        state = self.load_state()
        processed_anime = set(state.get('anime_processed_ids', []))
        processed_manga = set(state.get('manga_processed_ids', []))
        
        # Filter out already processed IDs
        anime_ids = [aid for aid in anime_ids if aid not in processed_anime]
        manga_ids = [mid for mid in manga_ids if mid not in processed_manga]
        
        print(f"üìä Smart Import Strategy:")
        print(f"   Anime IDs to process: {len(anime_ids):,} (discovered valid IDs)")
        print(f"   Manga IDs to process: {len(manga_ids):,} (discovered valid IDs)")
        print(f"   Total potential: {len(anime_ids) + len(manga_ids):,} items")
        print(f"   Expected success rate: 85-95% (vs 30-40% for sequential scanning)")
        print(f"   Processing: CONCURRENT with optimized batches")
        print()
        
        # Update counters from saved state
        self.anime_imported = state.get('anime_successful', 0)
        self.manga_imported = state.get('manga_successful', 0)
        self.anime_failed = state.get('anime_failed', 0)
        self.manga_failed = state.get('manga_failed', 0)
        
        if processed_anime or processed_manga:
            print(f"üîÑ RESUMING from previous session:")
            print(f"   Anime: {len(processed_anime):,} already processed (imported {self.anime_imported:,})")
            print(f"   Manga: {len(processed_manga):,} already processed (imported {self.manga_imported:,})")
            print()
        
        # Only run smart discovery if there are items to process
        if anime_ids or manga_ids:
            print("üéØ Starting SMART DISCOVERY import...")
            print("=" * 50)

            # Run anime and manga imports concurrently
            anime_task = asyncio.create_task(self.import_id_list_with_label(anime_ids, 'anime'))
            manga_task = asyncio.create_task(self.import_id_list_with_label(manga_ids, 'manga'))

            # Wait for both to complete
            await asyncio.gather(anime_task, manga_task)

            phase1_time = time.time() - start_time
            print(f"‚úÖ Phase 1 Complete! Duration: {phase1_time/3600:.1f} hours")
            print(f"   Smart Discovery: {self.anime_imported + self.manga_imported:,} items imported")
            print()
        else:
            print("‚úÖ Phase 1 already complete - all smart discovery items processed")
            print()
        
        # Phase 2: Sequential Scanning for comprehensive coverage
        print("üéØ Phase 2: Sequential Scanning for Complete Coverage")
        print("=" * 55)
        print("   This phase will scan ID ranges to find items missed by smart discovery")
        print("   Expected: 50K-100K additional items at 30-40% success rate")
        print()
        
        # Ask user if they want to continue with sequential scanning
        response = input("Continue with sequential scanning for complete MAL coverage? (y/N): ").strip().lower()
        if response != 'y':
            print("üõë Stopping after smart discovery phase")
            self.print_final_summary(start_time)
            return
        
        # Unified sequential scanning ranges (MAL uses single ID space for both anime and manga)
        # Resume from 9,000 to avoid reprocessing IDs we've already handled
        mal_id_ranges = [     # Recent era (medium density)
            (106700, 500000),   # Current era (lower density)
        ]
        
        print(f"üìä Sequential Scanning Plan:")
        print(f"   MAL ID ranges: {len(mal_id_ranges)} ranges covering IDs {mal_id_ranges[0][0]:,}-{mal_id_ranges[-1][1]:,}")
        print(f"   Each ID will be checked as anime first, then manga if not found")
        print(f"   Total potential: ~150,000 IDs to scan")
        print(f"   Expected finds: 50,000-100,000 additional items")
        print()
        
        # Start unified sequential scanning
        await self.start_unified_sequential_scanning(mal_id_ranges)
        
        # Final summary
        total_time = time.time() - start_time
        print()
        print("üéâ COMPREHENSIVE MAL Import Complete!")
        print("=" * 70)
        print(f"üìä Final Results:")
        print(f"   Anime: {self.anime_imported:,} imported")
        print(f"   Manga: {self.manga_imported:,} imported")
        print(f"   Total: {self.anime_imported + self.manga_imported:,} items imported")
        print(f"   Duration: {total_time/3600:.1f} hours")
        print(f"   Average Speed: {(self.anime_imported + self.manga_imported)/(total_time/3600):.0f} items/hour")
        print()
        print("üåê Complete MAL database now available in your cloud!")
        print("   Both local development and deployed versions can use this data.")
        
    def print_final_summary(self, start_time):
        """Print final summary for smart discovery only"""
        total_time = time.time() - start_time
        print()
        print("üéâ SMART DISCOVERY Import Complete!")
        print("=" * 50)
        print(f"üìä Results:")
        print(f"   Anime: {self.anime_imported:,} imported")
        print(f"   Manga: {self.manga_imported:,} imported")
        print(f"   Total: {self.anime_imported + self.manga_imported:,} items imported")
        print(f"   Duration: {total_time/3600:.1f} hours")
        print(f"   Speed: {(self.anime_imported + self.manga_imported)/(total_time/3600):.0f} items/hour")
        print()
        print("üåê Smart discovery data now available in your cloud!")
        
    async def start_unified_sequential_scanning(self, mal_id_ranges):
        """Start unified sequential ID scanning that determines anime vs manga from MAL API"""
        print("üîÑ Starting Unified Sequential ID Scanning...")
        print("   Each MAL ID will be checked as anime first, then manga")
        print("   This finds everything but takes longer due to dual API calls")
        print()
        
        # Process unified MAL ID ranges
        for i, (start_id, end_id) in enumerate(mal_id_ranges, 1):
            print(f"üéØ MAL ID Range {i}/{len(mal_id_ranges)}: IDs {start_id:,}-{end_id:,}")
            await self.scan_unified_id_range(start_id, end_id)
            print()
    
    async def scan_unified_id_range(self, start_id: int, end_id: int):
        """Scan a range of MAL IDs using unified import method"""
        batch_size = 1000  # Scan in chunks of 1000
        
        for batch_start in range(start_id, end_id + 1, batch_size):
            batch_end = min(batch_start + batch_size - 1, end_id)
            batch_ids = list(range(batch_start, batch_end + 1))
            
            print(f"   üì¶ Scanning MAL IDs {batch_start:,}-{batch_end:,}")
            
            # Use unified import that doesn't assume media type
            successful, failed, skipped = await self.import_batch_unified(batch_ids)
            
            # Count anime vs manga from successful imports
            success_rate = (successful / len(batch_ids)) * 100 if batch_ids else 0
            print(f"      ‚úÖ {successful} imported, ‚è≠Ô∏è {skipped} not found ({success_rate:.1f}% success)")
            print(f"      üìä Total imported: {self.anime_imported + self.manga_imported:,} items")
            
            # Save progress
            self.save_state()
    
    async def import_batch_unified(self, ids: List[int]) -> Tuple[int, int, int]:
        """Import batch using unified method that determines type from MAL API"""
        successful = 0
        failed = 0
        skipped = 0
        
        # Split into smaller concurrent groups
        semaphore = asyncio.Semaphore(self.concurrent_requests)
        
        async def import_single(item_id):
            async with semaphore:
                try:
                    # Use unified import method that determines actual type from MAL
                    success, message = await self.import_mal_item_unified(item_id)
                    
                    if success:
                        self.mal_requests_made = 0  # Reset request counter on success
                        # Update counters based on what was actually imported
                        if "anime" in message.lower():
                            self.anime_imported += 1
                        elif "manga" in message.lower():
                            self.manga_imported += 1
                        return 1, 0, 0  # success, failure, skipped
                    else:
                        # Check for timeout issues
                        if "timeout" in message.lower():
                            self.mal_requests_made += 1
                            if self.mal_requests_made >= self.max_retries:
                                # Apply progressive backoff
                                backoff_delay = min(5.0, self.mal_requests_made * 1.0)
                                print(f"         üêå Too many timeouts, backing off for {backoff_delay}s...")
                                await asyncio.sleep(backoff_delay)
                        
                        # Distinguish between fetch failures (normal) and actual errors
                        if "Failed to fetch" in message or "not found on MAL" in message:
                            # This is normal - item doesn't exist on MAL
                            return 0, 0, 1  # success, failure, skipped
                        elif "already exists" in message:
                            # Item already exists in database - this should be skipped, not counted as success
                            print(f"         ‚è≠Ô∏è  Item {item_id} already exists in database")
                            return 0, 0, 1  # success, failure, skipped (NOT SUCCESS!)
                        else:
                            # This is an actual error
                            print(f"         ‚ùå Error for item {item_id}: {message}")
                            return 0, 1, 0  # success, failure, skipped
                        
                except Exception as e:
                    return 0, 1, 0  # success, failure, skipped
                finally:
                    # Dynamic delay based on timeout situation
                    delay = self.delay_between_requests
                    if self.mal_requests_made > 0:
                        delay *= (1 + self.mal_requests_made * 0.5)  # Increase delay if timeouts
                    await asyncio.sleep(delay)
            
            # after every import_single()
            if self.mal_client.requests_since_cooldown > 250:      # getting close
                self.delay_between_requests = 0.12                 # slow down a bit
            elif self.mal_client.requests_since_cooldown < 150:    # plenty of room
                self.delay_between_requests = max(0.05, self.delay_between_requests - 0.01)
        
        # Run imports (now sequential due to MAL's rate limiting)
        tasks = [import_single(item_id) for item_id in ids]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Check if MAL client is in cooldown and report status
        if hasattr(self.mal_client, 'is_in_cooldown') and self.mal_client.is_in_cooldown:
            cooldown_time = time.time() - self.mal_client.cooldown_start_time
            print(f"      üïê MAL Rate Limit Cooldown Active ({cooldown_time:.0f}s elapsed)")
            print(f"      üìä Requests made in this cycle: {self.mal_client.requests_since_cooldown}")
        
        # Count results efficiently
        for result in results:
            if isinstance(result, tuple) and len(result) == 3:
                s, f, sk = result
                successful += s
                failed += f
                skipped += sk
            else:
                failed += 1  # Exception occurred
        
        return successful, failed, skipped

    async def import_mal_item_unified(self, mal_id: int) -> Tuple[bool, str]:
        """Import MAL item by determining its actual type (anime or manga) from MAL API"""
        try:
            # Check if item already exists in database first
            existing_check_response = await self._make_supabase_request_with_retry('GET', 'items', params={'select': 'id,uid,media_type_id', 'mal_id': f'eq.{mal_id}'})
            existing_items = existing_check_response.json()
            if existing_items:
                existing_type = "anime" if existing_items[0].get('media_type_id') == 1 else "manga"
                print(f"      ‚è≠Ô∏è  MAL ID {mal_id} already exists as {existing_type}, skipping")
                return False, f"MAL ID {mal_id} already exists (skipped)"
            
            # Try to fetch as anime first
            anime_data = await self.mal_client.get_anime_details(mal_id)
            if anime_data:
                print(f"      ‚úÖ MAL ID {mal_id} is anime: {anime_data.get('title', 'Unknown')}")
                return await self.import_anime_data(mal_id, anime_data)
            
            # If not anime, try as manga
            manga_data = await self.mal_client.get_manga_details(mal_id)
            if manga_data:
                print(f"      ‚úÖ MAL ID {mal_id} is manga: {manga_data.get('title', 'Unknown')}")
                return await self.import_manga_data(mal_id, manga_data)
            
            print(f"      ‚ùå MAL ID {mal_id} not found as anime or manga")
            return False, f"MAL ID {mal_id} not found on MAL"
                    
        except Exception as e:
            return False, f"Error importing MAL ID {mal_id}: {e}"

    async def import_data(self, mal_id: int, mal_data: dict, is_anime: bool) -> Tuple[bool, str]:
        """Generic data import function for both anime and manga."""
        media_type = "anime" if is_anime else "manga"
        
        # NSFW FILTERING: Check for Hentai genre only
        if any(genre.get('id') == 12 for genre in mal_data.get('genres', [])):
            print(f"      üö´ Skipping Hentai {media_type} {mal_id}")
            return False, f"Skipped Hentai {media_type} {mal_id}"

        # Transform data
        if is_anime:
            transformed = MALDataTransformer.transform_anime(mal_data)
        else:
            transformed = MALDataTransformer.transform_manga(mal_data)
        
        if not transformed:
            return False, f"Failed to transform {media_type} {mal_id}"

        # Prepare main item data payload
        categorized = self.categorize_mal_genres(mal_data.get('genres', []))
        alt_titles = mal_data.get('alternative_titles', {})
        main_picture = mal_data.get('main_picture', {})
        created_at_mal = mal_data.get('created_at', '').replace('Z', '+00:00') if mal_data.get('created_at') else None
        updated_at_mal = mal_data.get('updated_at', '').replace('Z', '+00:00') if mal_data.get('updated_at') else None

        item_data = {
            'uid': transformed['uid'], 'mal_id': transformed['mal_id'], 'title': transformed['title'],
            'synopsis': transformed.get('synopsis', ''), 'score': transformed.get('score', 0.0),
            'scored_by': transformed.get('scored_by', 0), 'popularity': transformed.get('popularity', 0),
            'status': transformed.get('status', ''), 'start_date': transformed.get('start_date'),
            'end_date': transformed.get('end_date'), 'image_url': transformed.get('image_url', ''),
            'alternative_titles_english': alt_titles.get('en', ''), 'alternative_titles_japanese': alt_titles.get('ja', ''),
            'alternative_titles_synonyms': alt_titles.get('synonyms', []) or [], 'main_picture_medium': main_picture.get('medium', ''),
            'main_picture_large': main_picture.get('large', ''), 'rank': mal_data.get('rank'),
            'num_list_users': mal_data.get('num_list_users'), 'num_scoring_users': mal_data.get('num_scoring_users'),
            'background': mal_data.get('background', ''), 'created_at_mal': created_at_mal, 'updated_at_mal': updated_at_mal,
        }

        if is_anime:
            start_season = mal_data.get('start_season', {})
            broadcast = mal_data.get('broadcast', {})
            item_data.update({
                'media_type_id': 1, 'episodes': transformed.get('episodes', 0),
                'media_type': mal_data.get('media_type', ''), 'source': mal_data.get('source', ''),
                'rating': mal_data.get('rating', ''), 'start_season_season': start_season.get('season', ''),
                'start_season_year': start_season.get('year'), 'broadcast_day': broadcast.get('day_of_the_week'),
                'broadcast_time': broadcast.get('start_time'), 'average_episode_duration': mal_data.get('average_episode_duration')
            })
        else:
            serialization = mal_data.get('serialization', [])
            item_data.update({
                'media_type_id': 2, 'chapters': transformed.get('chapters', 0), 'volumes': transformed.get('volumes', 0),
                'serialization': serialization[0].get('name', '') if serialization else ''
            })

        try:
            # Insert main item
            response = await self._make_supabase_request_with_retry('POST', 'items', data=item_data)
            result_data = response.json()
            item_uid = (result_data[0] if isinstance(result_data, list) else result_data).get('uid')
            
            if not item_uid:
                print(f"      ‚ö†Ô∏è  Unexpected response format for {media_type} {mal_id}: {result_data}")
                return False, f"Unexpected response format for {media_type} {mal_id}"

            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"      ‚úÖ [{timestamp}] Successfully inserted {media_type} {mal_id} (UID: {item_uid})")
            
            # Insert related data
            print(f"      üîó Inserting relationships for {media_type} {mal_id}...")
            await self.insert_enhanced_relationships_for_item(item_uid, categorized, transformed)
            print(f"      üìä Inserting rich data for {media_type} {mal_id}...")
            await self.insert_additional_rich_data(item_uid, mal_data, media_type)
            
            return True, f"Successfully imported {media_type} {mal_id}"
        except Exception as e:
            print(f"      ‚ùå Exception inserting {media_type} {mal_id}: {e}")
            return False, f"Failed to insert {media_type} {mal_id}: {e}"

    async def import_anime_data(self, mal_id: int, mal_data: dict) -> Tuple[bool, str]:
        """Wrapper for the generic import_data function for anime."""
        return await self.import_data(mal_id, mal_data, is_anime=True)
    
    async def import_manga_data(self, mal_id: int, mal_data: dict) -> Tuple[bool, str]:
        """Wrapper for the generic import_data function for manga."""
        return await self.import_data(mal_id, mal_data, is_anime=False)

    def categorize_mal_genres(self, mal_genres: List[Dict]) -> Dict[str, List[str]]:
        """Categorize MAL genres into proper genres, themes, and demographics"""
        categorized = {
            'genres': [],
            'themes': [],
            'demographics': []
        }
        
        for genre_item in mal_genres:
            genre_name = genre_item.get('name', '')
            genre_id = genre_item.get('id')
            
            # Try to categorize by ID first (most accurate)
            category_found = False
            for category, genre_dict in self.genre_categories.items():
                if genre_id in genre_dict:
                    categorized[category].append(genre_name)
                    category_found = True
                    break
            
            # If not found by ID, try by name (fallback)
            if not category_found:
                category = self.category_lookup.get(genre_name.lower(), 'genres')
                categorized[category].append(genre_name)
        
        return categorized

async def main():
    """Main function"""
    print("üöÄ MAL OPTIMIZED Smart Import Tool")
    print("=" * 60)
    print("This tool imports MAL data using intelligent ID discovery:")
    print("  üéØ Smart ID discovery via ranking & search APIs")
    print("  üöÄ 4-10x faster than sequential scanning")
    print("  üîÑ Transaction fixes for reliable imports")
    print("  üé¨ Anime and manga imported simultaneously")
    print()
    print("The data will be available to both:")
    print("  ‚Ä¢ Your local development environment")
    print("  ‚Ä¢ Your deployed production application")
    print()
    print("‚ú® Expected performance:")
    print("   ‚Ä¢ 85-95% success rate (vs 30-40% sequential)")
    print("   ‚Ä¢ ~20,000-40,000 items discovered and imported")
    print("   ‚Ä¢ 2-4 hours total runtime (vs 8-12 hours)")
    print()
    
    # Confirm with user
    response = input("Do you want to proceed with OPTIMIZED SMART MAL import? (y/N): ")
    if response.lower() != 'y':
        print("Import cancelled.")
        return
    
    print()
    
    # Start optimized smart import
    importer = OptimizedCloudMALImporter()
    await importer.start_optimized_smart_import()

if __name__ == "__main__":
    # Add graceful KeyboardInterrupt handling
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nüõë Import process interrupted by user. Exiting gracefully.")
        print("   Progress has been saved and can be resumed later.")