# backend/app.py
from flask import Flask, jsonify, request, g
from flask_cors import CORS
import os
import pandas as pd
from dotenv import load_dotenv
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from supabase_client import SupabaseClient, SupabaseAuthClient, require_auth
import requests
from datetime import datetime, timedelta
import json

load_dotenv()

app = Flask(__name__)
CORS(app)

# Global variables for data and ML models
df_processed = None
tfidf_vectorizer_global = None
tfidf_matrix_global = None
uid_to_idx = None
supabase_client = None
auth_client = None

# ADD a simple in-memory cache for media types
_media_type_cache = {}

def load_data_and_tfidf_from_supabase():
    """Load data from Supabase database instead of CSV files"""
    global df_processed, tfidf_vectorizer_global, tfidf_matrix_global, uid_to_idx, supabase_client

    if df_processed is not None and tfidf_matrix_global is not None:
        print(f"🔄 Data already loaded: {len(df_processed)} items")
        return

    try:
        print("🚀 Starting data load from Supabase...")
        
        # Initialize Supabase client
        supabase_client = SupabaseClient()
        
        # Get data as DataFrame from Supabase
        print("📊 Loading items from Supabase...")
        df_processed = supabase_client.items_to_dataframe(include_relations=True)
        
        print(f"✅ Loaded DataFrame with {len(df_processed)} items")
        
        if df_processed is None or len(df_processed) == 0:
            print("❌ No data loaded!")
            df_processed = pd.DataFrame()
            tfidf_vectorizer_global = None
            tfidf_matrix_global = None
            uid_to_idx = pd.Series(dtype='int64')
            return
        
        # Create combined text features for TF-IDF (if not exists)
        if 'combined_text_features' not in df_processed.columns:
            print("🔧 Creating combined text features...")
            df_processed = create_combined_text_features(df_processed)
            print(f"✅ Text features created. DataFrame now: {df_processed.shape}")
        
        # Fill NaN values in combined_text_features
        df_processed['combined_text_features'] = df_processed['combined_text_features'].fillna('')
        
        # Initialize TF-IDF vectorizer
        if len(df_processed) > 0:
            print("🤖 Creating TF-IDF matrix...")
            tfidf_vectorizer_global = TfidfVectorizer(stop_words='english', max_features=5000)
            tfidf_matrix_global = tfidf_vectorizer_global.fit_transform(df_processed['combined_text_features'])

            # Create UID to index mapping
            df_processed.reset_index(drop=True, inplace=True)
            uid_to_idx = pd.Series(df_processed.index, index=df_processed['uid'])
            print(f"✅ TF-IDF matrix created. Final data: {len(df_processed)} items")
        else:
            tfidf_vectorizer_global = None
            tfidf_matrix_global = None
            uid_to_idx = pd.Series(dtype='int64')
            
    except Exception as e:
        print(f"❌ Error loading data: {str(e)}")
        import traceback
        traceback.print_exc()
        df_processed = pd.DataFrame()
        tfidf_vectorizer_global = None
        tfidf_matrix_global = None
        uid_to_idx = pd.Series(dtype='int64')

def create_combined_text_features(df):
    """Create combined text features for TF-IDF if they don't exist"""
    
    def join_list_elements(lst):
        if isinstance(lst, list):
            return ' '.join(str(item).lower().replace(' ', '') for item in lst)
        return ''
    
    def preprocess_text(text):
        if pd.isna(text) or text is None:
            return ''
        return str(text).lower()
    
    # Create string versions of list columns (with safe column access)
    for col, default_str in [('genres', ''), ('themes', ''), ('demographics', '')]:
        if col in df.columns:
            df[f'{col}_str'] = df[col].apply(join_list_elements)
        else:
            df[f'{col}_str'] = default_str
    
    # Create start_year_num column if it doesn't exist
    if 'start_year_num' not in df.columns and 'start_date' in df.columns:
        # Extract year from start_date string (format: YYYY-MM-DD)
        df['start_year_num'] = df['start_date'].apply(
            lambda x: int(str(x)[:4]) if pd.notna(x) and str(x).strip() and len(str(x)) >= 4 and str(x)[:4].isdigit() else 0
        )
    elif 'start_year_num' not in df.columns:
        # Create empty column if neither exists
        df['start_year_num'] = 0
    
    # Create combined text features
    synopsis_str = df['synopsis'].apply(preprocess_text) if 'synopsis' in df.columns else ''
    title_str = df['title'].apply(preprocess_text) if 'title' in df.columns else ''
    
    df['combined_text_features'] = (
        df['genres_str'] + ' ' + 
        df['themes_str'] + ' ' + 
        df['demographics_str'] + ' ' + 
        synopsis_str + ' ' + 
        title_str
    )
    
    return df

def map_field_names_for_frontend(data_dict):
    """Map backend field names to frontend expected field names"""
    if isinstance(data_dict, dict):
        mapped_dict = data_dict.copy()
        
        # Map image_url to main_picture for consistency
        if 'image_url' in mapped_dict and 'main_picture' not in mapped_dict:
            mapped_dict['main_picture'] = mapped_dict['image_url']
        
        return mapped_dict
    return data_dict

def map_records_for_frontend(records_list):
    """Map field names for a list of records"""
    return [map_field_names_for_frontend(record) for record in records_list]

def initialize_auth_client():
    """Initialize the authentication client"""
    global auth_client
    if auth_client is None:
        auth_client = SupabaseAuthClient(
            os.getenv('SUPABASE_URL'),
            os.getenv('SUPABASE_KEY'),
            os.getenv('SUPABASE_SERVICE_KEY')
        )
    return auth_client

# Load data on startup
load_data_and_tfidf_from_supabase()

# Initialize auth client on startup
initialize_auth_client()

@app.route('/')
def hello():
    if df_processed is None or len(df_processed) == 0:
        return "Backend is initializing or no data available. Please check Supabase connection."
    return f"Hello from AniManga Recommender Backend! Loaded {len(df_processed)} items from Supabase."

@app.route('/api/items')
def get_items():
    if df_processed is None:
        return jsonify({"error": "Dataset not available."}), 503
    
    if len(df_processed) == 0:
        return jsonify({
            "items": [], "page": 1, "per_page": 30,
            "total_items": 0, "total_pages": 0, "sort_by": "score_desc"
        })

    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 30, type=int)
    search_query = request.args.get('q', None, type=str)
    media_type_filter = request.args.get('media_type', None, type=str)
    
    genre_filter_str = request.args.get('genre', None, type=str)
    theme_filter_str = request.args.get('theme', None, type=str)
    demographic_filter_str = request.args.get('demographic', None, type=str)
    studio_filter_str = request.args.get('studio', None, type=str)
    author_filter_str = request.args.get('author', None, type=str)
    
    status_filter = request.args.get('status', None, type=str)
    min_score_filter = request.args.get('min_score', None, type=float)
    year_filter = request.args.get('year', None, type=int)
    sort_by = request.args.get('sort_by', 'score_desc', type=str)

    data_subset = df_processed.copy()

    # Apply filters sequentially
    if search_query:
        data_subset = data_subset[data_subset['title'].fillna('').str.contains(search_query, case=False, na=False)]
    
    if media_type_filter and media_type_filter.lower() != 'all':
        data_subset = data_subset[data_subset['media_type'].fillna('').str.lower() == media_type_filter.lower()]
    
    def apply_multi_filter(df, column_name, filter_str_values):
        if filter_str_values and filter_str_values.lower() != 'all':
            selected_filters = [f.strip().lower() for f in filter_str_values.split(',') if f.strip()]
            if not selected_filters:
                return df
            
            def check_item_has_all_selected(item_column_list):
                if not isinstance(item_column_list, list): 
                    return False
                item_elements_lower = [str(elem).lower() for elem in item_column_list]
                return all(sel_filter in item_elements_lower for sel_filter in selected_filters)
            
            return df[df[column_name].apply(check_item_has_all_selected)]
        return df

    data_subset = apply_multi_filter(data_subset, 'genres', genre_filter_str)
    data_subset = apply_multi_filter(data_subset, 'themes', theme_filter_str)
    data_subset = apply_multi_filter(data_subset, 'demographics', demographic_filter_str)
    data_subset = apply_multi_filter(data_subset, 'studios', studio_filter_str)
    data_subset = apply_multi_filter(data_subset, 'authors', author_filter_str)
    
    if status_filter and status_filter.lower() != 'all':
        data_subset = data_subset[data_subset['status'].fillna('').str.lower() == status_filter.lower()]
    
    if min_score_filter is not None:
        data_subset = data_subset[pd.to_numeric(data_subset['score'], errors='coerce').fillna(-1) >= min_score_filter]
    
    if year_filter is not None:
        # Handle year filtering safely
        if 'start_year_num' in data_subset.columns:
            data_subset = data_subset[data_subset['start_year_num'] == year_filter]
        else:
            # If no start_year_num column, try to filter by start_date
            if 'start_date' in data_subset.columns:
                data_subset = data_subset[
                    data_subset['start_date'].str.startswith(str(year_filter), na=False)
                ]

    # Sorting
    if sort_by == 'score_desc':
        data_subset = data_subset.sort_values('score', ascending=False, na_position='last')
    elif sort_by == 'score_asc':
        data_subset = data_subset.sort_values('score', ascending=True, na_position='last')
    elif sort_by == 'title_asc':
        data_subset = data_subset.sort_values('title', ascending=True, na_position='last')
    elif sort_by == 'title_desc':
        data_subset = data_subset.sort_values('title', ascending=False, na_position='last')

    total_items = len(data_subset)
    total_pages = (total_items + per_page - 1) // per_page

    start_idx = (page - 1) * per_page
    end_idx = start_idx + per_page
    paginated_data = data_subset.iloc[start_idx:end_idx]

    # Convert to list of dicts for JSON response
    items_list = paginated_data.replace({np.nan: None}).to_dict(orient='records')
    
    # Map field names for frontend compatibility
    items_mapped = map_records_for_frontend(items_list)

    return jsonify({
        "items": items_mapped,
        "page": page,
        "per_page": per_page,
        "total_items": total_items,
        "total_pages": total_pages,
        "sort_by": sort_by
    })

@app.route('/api/distinct-values')
def get_distinct_values():
    if df_processed is None or len(df_processed) == 0:
        return jsonify({
            "genres": [], "statuses": [], "media_types": [], "themes": [],
            "demographics": [], "studios": [], "authors": []
        }), 503 if df_processed is None else 200
    
    try:
        def get_unique_from_lists(column_name):
            all_values = set()
            if column_name in df_processed.columns and not df_processed[column_name].dropna().empty:
                for item_list_val in df_processed[column_name].dropna():
                    if isinstance(item_list_val, list):
                        all_values.update(val.strip() for val in item_list_val if isinstance(val, str) and val.strip())
                    elif isinstance(item_list_val, str) and item_list_val.strip():
                        all_values.add(item_list_val.strip())
            return sorted(list(all_values))

        all_genres = get_unique_from_lists('genres')
        all_themes = get_unique_from_lists('themes')
        all_demographics = get_unique_from_lists('demographics')
        all_studios = get_unique_from_lists('studios')
        all_authors = get_unique_from_lists('authors')
        
        all_statuses = sorted(list(set(s.strip() for s in df_processed['status'].dropna().unique() if isinstance(s, str) and s.strip())))
        all_media_types = sorted(list(set(mt.strip() for mt in df_processed['media_type'].dropna().unique() if isinstance(mt, str) and mt.strip())))

        return jsonify({
            "genres": all_genres,
            "themes": all_themes,
            "demographics": all_demographics,
            "studios": all_studios,
            "authors": all_authors,
            "statuses": all_statuses,
            "media_types": all_media_types
        })
    except Exception as e:
        return jsonify({"error": "Could not retrieve distinct values."}), 500

@app.route('/api/items/<item_uid>')
def get_item_details(item_uid):
    if df_processed is None or uid_to_idx is None:
        return jsonify({"error": "Data not loaded or item UID mapping not available."}), 503
    
    if item_uid not in uid_to_idx.index:
        return jsonify({"error": "Item not found."}), 404
    
    idx = uid_to_idx[item_uid]
    item_details_series = df_processed.loc[idx].copy()
    item_details_series_cleaned = item_details_series.replace({np.nan: None})
    item_details_dict = item_details_series_cleaned.to_dict()
    
    # Map field names for frontend compatibility  
    item_details_mapped = map_field_names_for_frontend(item_details_dict)

    return jsonify(item_details_mapped)

@app.route('/api/recommendations/<item_uid>')
def get_recommendations(item_uid):
    if df_processed is None or tfidf_matrix_global is None or uid_to_idx is None:
        return jsonify({"error": "Recommendation system not ready. Data or TF-IDF matrix missing."}), 503

    if item_uid not in uid_to_idx.index:
        return jsonify({"error": "Target item for recommendations not found."}), 404
    
    try:
        item_idx = uid_to_idx[item_uid]
        source_title_value = df_processed.loc[item_idx, 'title']
        cleaned_source_title = None if pd.isna(source_title_value) else str(source_title_value)
        
        source_item_vector = tfidf_matrix_global[item_idx]
        sim_scores_for_item = cosine_similarity(source_item_vector, tfidf_matrix_global)
        sim_scores = list(enumerate(sim_scores_for_item[0]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        
        num_recommendations = request.args.get('n', 10, type=int)
        top_n_indices = [i[0] for i in sim_scores[1:num_recommendations+1]]

        recommended_items_df = df_processed.loc[top_n_indices].copy()
        recommended_items_for_json = recommended_items_df.replace({np.nan: None})
        recommended_list_of_dicts = recommended_items_for_json[['uid', 'title', 'media_type', 'score', 'image_url', 'genres', 'synopsis']].to_dict(orient='records')
        
        # Map field names for frontend compatibility
        recommended_mapped = map_records_for_frontend(recommended_list_of_dicts)

        return jsonify({
            "source_title": cleaned_source_title,
            "recommendations": recommended_mapped
        })
    except Exception as e:
        return jsonify({"error": "Could not generate recommendations."}), 500

@app.route('/api/auth/profile', methods=['GET'])
@require_auth
def get_user_profile():
    """Get current user's profile"""
    try:
        user_id = g.current_user['user_id']
        profile = auth_client.get_user_profile(user_id)
        
        if profile:
            return jsonify(profile)
        else:
            return jsonify({'error': 'Profile not found'}), 404
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/auth/profile', methods=['PUT'])
@require_auth
def update_user_profile():
    """Update current user's profile"""
    try:
        user_id = g.current_user['user_id']
        updates = request.json
        
        # Remove fields that shouldn't be updated directly
        updates.pop('id', None)
        updates.pop('created_at', None)
        
        profile = auth_client.update_user_profile(user_id, updates)
        
        if profile:
            return jsonify(profile)
        else:
            return jsonify({'error': 'Failed to update profile'}), 400
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500
    

@app.route('/api/auth/dashboard', methods=['GET'])
@require_auth
def get_user_dashboard():
    """Get user's complete dashboard data"""
    user_id = g.current_user['user_id'] if 'user_id' in g.current_user else g.current_user['sub']
    
    print(f"🎯 Dashboard request for user_id: {user_id}")  # Debug log
    
    try:
        dashboard_data = {
            'user_stats': get_user_statistics(user_id),
            'recent_activity': get_recent_user_activity(user_id),
            'in_progress': get_user_items_by_status(user_id, 'watching'),
            'completed_recently': get_recently_completed(user_id),
            'plan_to_watch': get_user_items_by_status(user_id, 'plan_to_watch'),
            'on_hold': get_user_items_by_status(user_id, 'on_hold'),
            'quick_stats': get_quick_stats(user_id)
        }
        
        print(f"🎯 Dashboard data summary: {dashboard_data.get('quick_stats', {}).get('completed', 0)} completed items")
        
        return jsonify(dashboard_data)
    except Exception as e:
        print(f"Error getting dashboard data: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to load dashboard data'}), 500

@app.route('/api/auth/user-items', methods=['GET'])
@require_auth
def get_user_items():
    """Get user's anime/manga list"""
    try:
        user_id = g.current_user['user_id']
        status = request.args.get('status')  # Optional filter by status
        
        items = auth_client.get_user_items(user_id, status)
        return jsonify(items)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/auth/user-items/<item_uid>', methods=['POST', 'PUT'])
@require_auth
def update_user_item_status(item_uid):
    """Update user's status for an anime/manga item - WITH COMPLETION DATE SUPPORT"""
    try:
        user_id = g.current_user['user_id'] if 'user_id' in g.current_user else g.current_user['sub']
        data = request.json
        
        print(f"🎯 Updating item {item_uid} for user {user_id}")
        print(f"📝 Received data: {data}")
        
        # Extract required data
        status = data.get('status')
        if not status:
            return jsonify({'error': 'Status is required'}), 400
        
        rating = data.get('rating')
        progress = data.get('progress', 0)
        notes = data.get('notes', '')
        completion_date = data.get('completion_date')  # ✅ NEW: Get completion_date from frontend
        
        # AUTO-COMPLETION LOGIC: Set progress to max when completed
        if status == 'completed' and progress == 0:
            # Get item details to determine max progress
            if item_uid in uid_to_idx.index:
                idx = uid_to_idx[item_uid]
                item_details = df_processed.loc[idx]
                
                if item_details['media_type'] == 'anime':
                    max_progress = int(item_details.get('episodes', 1) or 1)
                else:  # manga
                    max_progress = int(item_details.get('chapters', 1) or 1)
                
                progress = max_progress
                print(f"🎯 Auto-setting progress to {max_progress} for completed {item_details['media_type']}")
            else:
                progress = 1  # Fallback
        
        # Create comprehensive status data
        status_data = {
            'status': status,
            'progress': progress,
            'notes': notes
        }
        
        if rating is not None and rating > 0:
            status_data['rating'] = rating
        
        # ✅ NEW: Add completion_date if provided
        if completion_date:
            status_data['completion_date'] = completion_date
        
        print(f"📤 Sending to Supabase: {status_data}")
        
        # Call the enhanced update method
        result = auth_client.update_user_item_status_comprehensive(user_id, item_uid, status_data)
        
        if result and result.get('success'):
            print(f"✅ Update successful!")
            
            # ✅ INVALIDATE CACHE so next dashboard load will be fresh
            invalidate_user_statistics_cache(user_id)
            
            # Log activity for dashboard updates
            log_user_activity(user_id, 'status_changed', item_uid, {
                'new_status': status,
                'progress': progress
            })
            
            return jsonify({'success': True, 'data': result.get('data', {})})
        else:
            print(f"❌ Update failed: {result}")
            return jsonify({'error': 'Failed to update item status'}), 400
            
    except Exception as e:
        print(f"❌ Error in update_user_item_status: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/auth/user-items/<item_uid>', methods=['DELETE'])
@require_auth
def remove_user_item(item_uid):
    """Remove item from user's list"""
    try:
        user_id = g.current_user['user_id']
        
        response = requests.delete(
            f"{os.getenv('SUPABASE_URL')}/rest/v1/user_items",
            headers=auth_client.headers,
            params={
                'user_id': f'eq.{user_id}',
                'item_uid': f'eq.{item_uid}'
            }
        )
        
        if response.status_code == 204:
            return jsonify({'message': 'Item removed successfully'})
        else:
            return jsonify({'error': 'Failed to remove item'}), 400
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/auth/user-items/by-status/<status>', methods=['GET'])
@require_auth
def get_user_items_by_status_route(status):
    """Get user's items filtered by status"""
    user_id = g.current_user['user_id'] if 'user_id' in g.current_user else g.current_user['sub']
    
    try:
        items = get_user_items_by_status(user_id, status)
        return jsonify({'items': items, 'count': len(items)})
    except Exception as e:
        print(f"Error getting items by status: {e}")
        return jsonify({'error': 'Failed to get items'}), 500

@app.route('/api/auth/verify-token', methods=['GET'])
@require_auth
def verify_token():
    """Verify if the current token is valid"""
    return jsonify({
        'valid': True,
        'user': g.current_user
    })

@app.route('/api/auth/force-refresh-stats', methods=['POST'])
@require_auth
def force_refresh_statistics():
    """Force recalculation of user statistics"""
    try:
        user_id = g.current_user['user_id'] if 'user_id' in g.current_user else g.current_user['sub']
        
        print(f"🔄 Force refreshing statistics for user {user_id}")
        
        # Calculate fresh statistics
        fresh_stats = calculate_user_statistics_realtime(user_id)
        
        if not fresh_stats:
            return jsonify({'error': 'Failed to calculate statistics'}), 500
        
        print(f"📊 Fresh stats: {fresh_stats}")
        
        # Delete existing statistics first
        delete_response = requests.delete(
            f"{auth_client.base_url}/rest/v1/user_statistics",
            headers=auth_client.headers,
            params={'user_id': f'eq.{user_id}'}
        )
        
        # Insert new statistics
        insert_response = requests.post(
            f"{auth_client.base_url}/rest/v1/user_statistics",
            headers=auth_client.headers,
            json=fresh_stats
        )
        
        if insert_response.status_code in [200, 201]:
            return jsonify({'success': True, 'statistics': fresh_stats})
        else:
            return jsonify({'error': 'Failed to update statistics'}), 500
            
    except Exception as e:
        print(f"Error force refreshing statistics: {e}")
        return jsonify({'error': str(e)}), 500

#helper functions
def get_user_statistics(user_id: str) -> dict:
    """Get user statistics - SMART CACHING with auto-invalidation"""
    try:
        print(f"📊 Getting statistics for user {user_id}")
        
        # Try to get cached statistics first
        cached_stats = get_cached_user_statistics(user_id)
        
        if cached_stats and is_cache_fresh(cached_stats):
            print(f"⚡ Using cached statistics (fresh)")
            return cached_stats
        
        print(f"🔄 Cache miss or stale, calculating fresh statistics")
        
        # Calculate fresh statistics
        fresh_stats = calculate_user_statistics_realtime(user_id)
        
        if fresh_stats:
            # Update cache with fresh data
            update_user_statistics_cache(user_id, fresh_stats)
            print(f"✅ Cache updated with fresh stats: anime={fresh_stats.get('total_anime_watched')}, manga={fresh_stats.get('total_manga_read')}")
            return fresh_stats
        
        # Fallback to cached data even if stale
        if cached_stats:
            print(f"⚠️ Using stale cached data as fallback")
            return cached_stats
        
        # Final fallback to defaults
        print("⚠️ No data available, returning defaults")
        return get_default_user_statistics()
        
    except Exception as e:
        print(f"Error getting user statistics: {e}")
        return get_default_user_statistics()

def get_cached_user_statistics(user_id: str) -> dict:
    """Get cached user statistics from database"""
    try:
        response = requests.get(
            f"{auth_client.base_url}/rest/v1/user_statistics",
            headers=auth_client.headers,
            params={'user_id': f'eq.{user_id}'}
        )
        
        if response.status_code == 200:
            data = response.json()
            if data:
                return data[0]
        return None
    except Exception as e:
        print(f"Error getting cached statistics: {e}")
        return None

def is_cache_fresh(cached_stats: dict, max_age_minutes: int = 5) -> bool:
    """Check if cached statistics are fresh enough"""
    try:
        if not cached_stats.get('updated_at'):
            return False
        
        from datetime import datetime, timedelta
        updated_at = datetime.fromisoformat(cached_stats['updated_at'].replace('Z', '+00:00'))
        now = datetime.now().replace(tzinfo=updated_at.tzinfo)
        age = now - updated_at
        
        is_fresh = age < timedelta(minutes=max_age_minutes)
        print(f"🕒 Cache age: {age}, fresh: {is_fresh}")
        return is_fresh
    except Exception as e:
        print(f"Error checking cache freshness: {e}")
        return False

def update_user_statistics_cache(user_id: str, stats: dict):
    """Update user statistics cache with upsert"""
    try:
        # Add timestamp and user_id
        cache_data = {
            **stats,
            'user_id': str(user_id),
            'updated_at': datetime.now().isoformat()
        }
        
        # Use UPSERT to update existing or insert new
        response = requests.post(
            f"{auth_client.base_url}/rest/v1/user_statistics",
            headers={
                **auth_client.headers,
                'Prefer': 'resolution=merge-duplicates'  # Enable upsert
            },
            json=cache_data
        )
        
        if response.status_code in [200, 201]:
            print(f"✅ Statistics cache updated successfully")
            return True
        else:
            print(f"❌ Failed to update cache: {response.status_code} - {response.text}")
            return False
            
    except Exception as e:
        print(f"Error updating statistics cache: {e}")
        return False

def invalidate_user_statistics_cache(user_id: str):
    """Invalidate user statistics cache by deleting it"""
    try:
        print(f"🗑️ Invalidating statistics cache for user {user_id}")
        
        response = requests.delete(
            f"{auth_client.base_url}/rest/v1/user_statistics",
            headers=auth_client.headers,
            params={'user_id': f'eq.{user_id}'}
        )
        
        return response.status_code in [200, 204]
    except Exception as e:
        print(f"Error invalidating cache: {e}")
        return False

def get_default_user_statistics() -> dict:
    """Get default user statistics"""
    return {
        'total_anime_watched': 0,
        'total_manga_read': 0,
        'total_hours_watched': 0.0,
        'total_chapters_read': 0,
        'average_score': 0.0,
        'favorite_genres': [],
        'current_streak_days': 0,
        'longest_streak_days': 0,
        'completion_rate': 0.0
    }

def calculate_user_statistics_realtime(user_id: str) -> dict:
    """Calculate user statistics in real-time - NO CACHING"""
    try:
        # Get all user items directly
        response = requests.get(
            f"{auth_client.base_url}/rest/v1/user_items",
            headers=auth_client.headers,
            params={'user_id': f'eq.{user_id}'}
        )
        
        if response.status_code != 200:
            print(f"❌ Failed to get user items: {response.status_code}")
            return {}
        
        user_items = response.json()
        print(f"📝 Found {len(user_items)} total user items")
        
        completed_items = [item for item in user_items if item['status'] == 'completed']
        print(f"✅ Found {len(completed_items)} completed items")
        
        # Count anime vs manga in completed items
        anime_count = 0
        manga_count = 0
        
        for item in completed_items:
            media_type = get_item_media_type(item['item_uid'])
            print(f"🎯 Item {item['item_uid']}: media_type = {media_type}")
            
            if media_type == 'anime':
                anime_count += 1
            elif media_type == 'manga':
                manga_count += 1
        
        print(f"📊 Final counts: anime={anime_count}, manga={manga_count}")
        
        # Calculate enhanced statistics with proper type conversion
        stats = {
            'total_anime_watched': int(anime_count),
            'total_manga_read': int(manga_count),
            'total_hours_watched': float(calculate_watch_time(completed_items)),
            'total_chapters_read': int(calculate_chapters_read(completed_items)),
            'average_score': float(calculate_average_user_score(user_items)),
            'favorite_genres': list(get_user_favorite_genres(user_items)),
            'current_streak_days': int(calculate_current_streak(user_id)),
            'longest_streak_days': int(calculate_longest_streak(user_id)),
            'completion_rate': float(calculate_completion_rate(user_items))
        }
        
        return stats
    except Exception as e:
        print(f"Error calculating real-time statistics: {e}")
        import traceback
        traceback.print_exc()
        return {}

def get_recent_user_activity(user_id: str, limit: int = 10) -> list:
    """Get user's recent activity"""
    try:
        response = requests.get(
            f"{auth_client.base_url}/rest/v1/user_activity",
            headers=auth_client.headers,
            params={
                'user_id': f'eq.{user_id}',
                'order': 'created_at.desc',
                'limit': limit
            }
        )
        
        if response.status_code == 200:
            activities = response.json()
            # Enrich with item details
            for activity in activities:
                item_details = get_item_details_simple(activity['item_uid'])
                activity['item'] = item_details
            return activities
        return []
    except Exception as e:
        print(f"Error getting recent activity: {e}")
        return []

def get_user_items_by_status(user_id: str, status: str, limit: int = 20) -> list:
    """Get user's items by status"""
    try:
        response = requests.get(
            f"{auth_client.base_url}/rest/v1/user_items",
            headers=auth_client.headers,
            params={
                'user_id': f'eq.{user_id}',
                'status': f'eq.{status}',
                'order': 'updated_at.desc',
                'limit': limit
            }
        )
        
        if response.status_code == 200:
            user_items = response.json()
            # Enrich with item details
            for user_item in user_items:
                item_details = get_item_details_simple(user_item['item_uid'])
                user_item['item'] = item_details
            return user_items
        return []
    except Exception as e:
        print(f"Error getting items by status: {e}")
        return []

def get_recently_completed(user_id: str, days: int = 30, limit: int = 10) -> list:
    """Get recently completed items"""
    try:
        since_date = (datetime.now() - timedelta(days=days)).isoformat()
        
        response = requests.get(
            f"{auth_client.base_url}/rest/v1/user_items",
            headers=auth_client.headers,
            params={
                'user_id': f'eq.{user_id}',
                'status': 'eq.completed',
                'completion_date': f'gte.{since_date}',
                'order': 'completion_date.desc',
                'limit': limit
            }
        )
        
        if response.status_code == 200:
            items = response.json()
            # Enrich with item details
            for item in items:
                item_details = get_item_details_simple(item['item_uid'])
                item['item'] = item_details
            return items
        return []
    except Exception as e:
        print(f"Error getting recently completed: {e}")
        return []

def get_quick_stats(user_id: str) -> dict:
    """Get quick stats for dashboard"""
    try:
        response = requests.get(
            f"{auth_client.base_url}/rest/v1/user_items",
            headers=auth_client.headers,
            params={'user_id': f'eq.{user_id}', 'select': 'status'}
        )
        
        if response.status_code == 200:
            items = response.json()
            stats = {
                'total_items': len(items),
                'watching': len([i for i in items if i['status'] == 'watching']),
                'completed': len([i for i in items if i['status'] == 'completed']),
                'plan_to_watch': len([i for i in items if i['status'] == 'plan_to_watch']),
                'on_hold': len([i for i in items if i['status'] == 'on_hold']),
                'dropped': len([i for i in items if i['status'] == 'dropped'])
            }
            return stats
        return {}
    except Exception as e:
        print(f"Error getting quick stats: {e}")
        return {}

def log_user_activity(user_id: str, activity_type: str, item_uid: str, activity_data: dict = None):
    """Log user activity"""
    try:
        data = {
            'user_id': user_id,
            'activity_type': activity_type,
            'item_uid': item_uid,
            'activity_data': activity_data or {}
        }
        
        response = requests.post(
            f"{auth_client.base_url}/rest/v1/user_activity",
            headers=auth_client.headers,
            json=data
        )
        
        return response.status_code in [200, 201]
    except Exception as e:
        print(f"Error logging activity: {e}")
        return False

def calculate_watch_time(completed_items: list) -> float:
    """Calculate total watch time in hours using actual episode data"""
    total_minutes = 0.0
    
    try:
        for item in completed_items:
            if get_item_media_type(item['item_uid']) == 'anime':
                # Get actual episode count and duration from the item
                item_details = get_item_details_for_stats(item['item_uid'])
                if item_details:
                    episodes = item_details.get('episodes', 0) or 0
                    # Use actual duration if available, otherwise default to 24 minutes
                    duration_per_episode = item_details.get('duration_minutes', 24) or 24
                    total_minutes += episodes * duration_per_episode
                else:
                    # Fallback: assume 24 minutes per episode if no data
                    total_minutes += 24
        
        return round(total_minutes / 60.0, 1)  # Convert to hours
    except Exception as e:
        print(f"Error calculating watch time: {e}")
        return 0.0

def calculate_chapters_read(completed_items: list) -> int:
    """Calculate total chapters read using actual manga data"""
    total_chapters = 0
    
    try:
        for item in completed_items:
            if get_item_media_type(item['item_uid']) == 'manga':
                # Get actual chapter count from the item
                item_details = get_item_details_for_stats(item['item_uid'])
                if item_details:
                    chapters = item_details.get('chapters', 0) or 0
                    total_chapters += chapters
                else:
                    # Fallback: assume 1 chapter if no data
                    total_chapters += 1
        
        return total_chapters
    except Exception as e:
        print(f"Error calculating chapters read: {e}")
        return 0

def get_user_favorite_genres(user_items: list) -> list:
    """Get user's most common genres from their actual items"""
    genre_counts = {}
    
    try:
        for user_item in user_items:
            # Get item details to access genres
            item_details = get_item_details_for_stats(user_item['item_uid'])
            if item_details and item_details.get('genres'):
                for genre in item_details['genres']:
                    if isinstance(genre, str) and genre.strip():
                        genre_counts[genre] = genre_counts.get(genre, 0) + 1
        
        # Sort by frequency and return top 5
        sorted_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)
        return [genre for genre, count in sorted_genres[:5]]
    except Exception as e:
        print(f"Error getting favorite genres: {e}")
        return []

def calculate_current_streak(user_id: str) -> int:
    """Calculate current consecutive days of activity"""
    try:
        # Get recent activity, ordered by date
        response = requests.get(
            f"{auth_client.base_url}/rest/v1/user_activity",
            headers=auth_client.headers,
            params={
                'user_id': f'eq.{user_id}',
                'order': 'created_at.desc',
                'limit': 100  # Get enough data to calculate streak
            }
        )
        
        if response.status_code != 200:
            return 0
            
        activities = response.json()
        if not activities:
            return 0
        
        # Group activities by date
        from datetime import datetime, timedelta
        activity_dates = set()
        
        for activity in activities:
            created_at = datetime.fromisoformat(activity['created_at'].replace('Z', '+00:00'))
            activity_date = created_at.date()
            activity_dates.add(activity_date)
        
        if not activity_dates:
            return 0
        
        # Sort dates in descending order
        sorted_dates = sorted(activity_dates, reverse=True)
        
        # Calculate consecutive streak from today
        today = datetime.now().date()
        current_streak = 0
        expected_date = today
        
        for activity_date in sorted_dates:
            if activity_date == expected_date:
                current_streak += 1
                expected_date = expected_date - timedelta(days=1)
            elif activity_date == expected_date + timedelta(days=1):
                # Allow for today not having activity yet
                current_streak += 1
                expected_date = expected_date - timedelta(days=1)
            else:
                break
        
        return current_streak
    except Exception as e:
        print(f"Error calculating current streak: {e}")
        return 0

def calculate_longest_streak(user_id: str) -> int:
    """Calculate longest consecutive days streak in user history"""
    try:
        # Get all user activity
        response = requests.get(
            f"{auth_client.base_url}/rest/v1/user_activity",
            headers=auth_client.headers,
            params={
                'user_id': f'eq.{user_id}',
                'order': 'created_at.asc'  # Oldest first for historical analysis
            }
        )
        
        if response.status_code != 200:
            return 0
            
        activities = response.json()
        if not activities:
            return 0
        
        # Group activities by date
        from datetime import datetime, timedelta
        activity_dates = set()
        
        for activity in activities:
            created_at = datetime.fromisoformat(activity['created_at'].replace('Z', '+00:00'))
            activity_date = created_at.date()
            activity_dates.add(activity_date)
        
        if not activity_dates:
            return 0
        
        # Sort dates and find longest consecutive streak
        sorted_dates = sorted(activity_dates)
        longest_streak = 1
        current_streak = 1
        
        for i in range(1, len(sorted_dates)):
            expected_date = sorted_dates[i-1] + timedelta(days=1)
            if sorted_dates[i] == expected_date:
                current_streak += 1
                longest_streak = max(longest_streak, current_streak)
            else:
                current_streak = 1
        
        return longest_streak
    except Exception as e:
        print(f"Error calculating longest streak: {e}")
        return 0

def get_item_details_for_stats(item_uid: str) -> dict:
    """Get item details for statistics calculations (JSON SAFE VERSION)"""
    try:
        if df_processed is not None and not df_processed.empty:
            item_row = df_processed[df_processed['uid'] == item_uid]
            if not item_row.empty:
                item = item_row.iloc[0]
                
                # Convert numpy types to native Python types
                return {
                    'uid': str(item['uid']),
                    'title': str(item['title']),
                    'media_type': str(item['media_type']),
                    'episodes': int(item.get('episodes', 0)) if pd.notna(item.get('episodes')) else 0,
                    'chapters': int(item.get('chapters', 0)) if pd.notna(item.get('chapters')) else 0,
                    'duration_minutes': int(item.get('duration_minutes', 24)) if pd.notna(item.get('duration_minutes')) else 24,
                    'genres': list(item.get('genres', [])) if isinstance(item.get('genres'), list) else [],
                    'score': float(item.get('score', 0)) if pd.notna(item.get('score')) else 0.0
                }
        return {}
    except Exception as e:
        print(f"Error getting item details for stats: {e}")
        return {}

def calculate_user_statistics(user_id: str) -> dict:
    """Calculate comprehensive user statistics - JSON SAFE VERSION"""
    try:
        # Get all user items
        response = requests.get(
            f"{auth_client.base_url}/rest/v1/user_items",
            headers=auth_client.headers,
            params={'user_id': f'eq.{user_id}'}
        )
        
        if response.status_code != 200:
            return {}
        
        user_items = response.json()
        completed_items = [item for item in user_items if item['status'] == 'completed']
        
        # Calculate enhanced statistics with proper type conversion
        stats = {
            'user_id': str(user_id),
            'total_anime_watched': int(len([item for item in completed_items if get_item_media_type(item['item_uid']) == 'anime'])),
            'total_manga_read': int(len([item for item in completed_items if get_item_media_type(item['item_uid']) == 'manga'])),
            'total_hours_watched': float(calculate_watch_time(completed_items)),
            'total_chapters_read': int(calculate_chapters_read(completed_items)),
            'average_score': float(calculate_average_user_score(user_items)),
            'favorite_genres': list(get_user_favorite_genres(user_items)),
            'current_streak_days': int(calculate_current_streak(user_id)),
            'longest_streak_days': int(calculate_longest_streak(user_id)),
            'completion_rate': float(calculate_completion_rate(user_items)),
            'updated_at': datetime.now().isoformat()
        }
        
        return stats
    except Exception as e:
        print(f"Error calculating enhanced statistics: {e}")
        return {}

def calculate_average_user_score(user_items: list) -> float:
    """Calculate user's average rating for items they've scored"""
    try:
        scored_items = [item for item in user_items if item.get('rating') and item['rating'] > 0]
        if not scored_items:
            return 0.0
        
        total_score = sum(item['rating'] for item in scored_items)
        return round(total_score / len(scored_items), 2)
    except Exception as e:
        print(f"Error calculating average score: {e}")
        return 0.0

def calculate_completion_rate(user_items: list) -> float:
    """Calculate completion rate percentage"""
    if not user_items:
        return 0.0
    
    completed = len([item for item in user_items if item['status'] == 'completed'])
    total = len(user_items)
    return round((completed / total) * 100, 2) if total > 0 else 0.0

# ENHANCE the get_item_media_type function:
def get_item_media_type(item_uid: str) -> str:
    """Get item media type (anime/manga) - WITH CACHING"""
    try:
        # Check in-memory cache first
        if item_uid in _media_type_cache:
            return _media_type_cache[item_uid]
        
        if df_processed is not None and not df_processed.empty:
            item_row = df_processed[df_processed['uid'] == item_uid]
            if not item_row.empty:
                media_type = item_row.iloc[0].get('media_type', 'unknown')
                # Cache the result
                _media_type_cache[item_uid] = media_type
                print(f"🔍 Item {item_uid}: media_type = {media_type} (cached)")
                return media_type
            else:
                print(f"⚠️ Item {item_uid} not found in df_processed")
        else:
            print(f"⚠️ df_processed is None or empty")
        
        # Cache unknown results too
        _media_type_cache[item_uid] = 'unknown'
        return 'unknown'
    except Exception as e:
        print(f"Error getting item media type for {item_uid}: {e}")
        return 'unknown'

# Add this to get item details for frontend calculations
def get_item_details_simple(item_uid: str) -> dict:
    """Get basic item details for API responses - JSON SAFE VERSION"""
    try:
        if df_processed is not None and not df_processed.empty:
            item_row = df_processed[df_processed['uid'] == item_uid]
            if not item_row.empty:
                item = item_row.iloc[0]
                
                # Convert numpy types to native Python types for JSON serialization
                return {
                    'uid': str(item['uid']),
                    'title': str(item['title']),
                    'media_type': str(item['media_type']),
                    'episodes': int(item.get('episodes', 0)) if pd.notna(item.get('episodes')) else None,
                    'chapters': int(item.get('chapters', 0)) if pd.notna(item.get('chapters')) else None,
                    'score': float(item.get('score', 0)) if pd.notna(item.get('score')) else 0.0,
                    'image_url': str(item.get('image_url', '')) if pd.notna(item.get('image_url')) else None
                }
        return {}
    except Exception as e:
        print(f"Error getting item details: {e}")
        return {}

if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=5000)